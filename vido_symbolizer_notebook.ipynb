{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b083d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\s4582742\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "WARNING:tensorflow:From C:\\Users\\s4582742\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\deprecation.py:602: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Takes too long to run on a single video. Plus barely extracts any features.\n",
    "    Pick specific videos that have good frontal views, and extract symbols from\n",
    "    ~ 5000 frames per video.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "ROOT_DIR = \"\"\n",
    "SORT_DIR = \"\"\n",
    "sys.path.append(ROOT_DIR) # Adding MRCNN root dir to path to import models\n",
    "sys.path.append(SORT_DIR) # Adding sort to path to import it\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage.color import rgba2rgb\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from mrcnn import config # Importing RCNN\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "import sort # Importing SORT\n",
    "\n",
    "class InferenceConfig(config.Config):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    NAME = 'coco'\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 80  # COCO has 80 classes\n",
    "\n",
    "def get_average(landmarks):\n",
    "    \"\"\"\n",
    "    Function calculates the average position of a list of landmarks\n",
    "\n",
    "    Parameters\n",
    "        - landmarks (list): List of landmarks\n",
    "    Return\n",
    "        A tuple with the average x and y coordinates for a given list of\n",
    "        landmarks\n",
    "    \"\"\"\n",
    "    avg_x = 0\n",
    "    avg_y = 0\n",
    "    for landmark in landmarks:\n",
    "        avg_x += landmark.x\n",
    "        avg_y += landmark.y\n",
    "    return avg_x/len(landmarks), avg_y/len(landmarks)\n",
    "\n",
    "def get_landmark_values(landmark_enums, results):\n",
    "    \"\"\"\n",
    "    Returns x, y coordinates of the landmark values corresponding to the\n",
    "    landmark_enums\n",
    "\n",
    "    Parameters\n",
    "        - landmark_enums (list): A list of enums for the landmarks\n",
    "        - results : the return value from mediapipe's mp_pose.Pose.process()\n",
    "    Returns\n",
    "        The x, y coordinates of the average position of the landmarks passed\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for enum in landmark_enums:\n",
    "        values.append(results.pose_landmarks.landmark[enum])\n",
    "    return get_average(values)\n",
    "\n",
    "# Initializing some global variables\n",
    "VIDEO_FOLDER = \"C:\\\\Users\\\\s4582742\\\\Downloads\\\\videos\"\n",
    "OUTPUT_FOLDER = \"./output2\"\n",
    "VIS = False#\"--vis\" in sys.argv\n",
    "OUTPUT = True#\"--output\" in sys.argv\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Read videos\n",
    "video_paths = [file for file in os.listdir(VIDEO_FOLDER)\n",
    "                if file.endswith(\".mp4\") or file.endswith(\".mkv\")]\n",
    "num_vids = len(video_paths)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "# Getting Enums for the landmarks we are interested in\n",
    "hand_landmarks_left = [mp_pose.PoseLandmark.LEFT_PINKY,\n",
    "                       mp_pose.PoseLandmark.LEFT_INDEX,\n",
    "                       mp_pose.PoseLandmark.LEFT_THUMB]\n",
    "\n",
    "hand_landmarks_right = [mp_pose.PoseLandmark.RIGHT_PINKY,\n",
    "                        mp_pose.PoseLandmark.RIGHT_INDEX,\n",
    "                        mp_pose.PoseLandmark.RIGHT_THUMB]\n",
    "\n",
    "mouth_landmarks = [mp_pose.PoseLandmark.MOUTH_LEFT,\n",
    "                   mp_pose.PoseLandmark.MOUTH_RIGHT]\n",
    "\n",
    "eye_landmarks = [mp_pose.PoseLandmark.LEFT_EYE,\n",
    "                 mp_pose.PoseLandmark.RIGHT_EYE]\n",
    "\n",
    "shoulder_landmarks = [mp_pose.PoseLandmark.LEFT_SHOULDER,\n",
    "                      mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "\n",
    "hip_landmarks = [mp_pose.PoseLandmark.LEFT_HIP,\n",
    "                 mp_pose.PoseLandmark.RIGHT_HIP]\n",
    "\n",
    "conf = InferenceConfig()\n",
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\",\n",
    "                          model_dir=MODEL_DIR,\n",
    "                          config=conf)\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "symbols = {\"A\":(), \"B\":(), \"C\":(), \"D\":(), \"E\":(), \"F\":(), \"G\":()}\n",
    "if VIS:\n",
    "    colors = {\"A\":(225, 0, 0),\n",
    "              \"B\":(0, 225, 0),\n",
    "              \"C\":(0, 0, 225),\n",
    "              \"D\":(225, 225, 0),\n",
    "              \"E\":(0, 225, 225),\n",
    "              \"F\":(225, 0, 225),\n",
    "              \"G\":(225, 225, 225)}\n",
    "\n",
    "check = lambda limits, pos: (pos < limits[0]) and (pos > limits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67b580c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Symbolization Process\n",
      "\n",
      "Loading in Video 0sblzcxLTcI.mp4 (0 of 37)\n",
      "\r",
      "[..................................................] 0.00% 0/20"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s4582742\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.00% 20/20\n",
      "Loading in Video 24Tcr-xjV4U.mp4 (1 of 37)\n",
      "[==================================================] 100.00% 20/20\n",
      "Loading in Video 4F724-kGoGE.mkv (2 of 37)\n",
      "[==================================================] 100.00% 20/20\n",
      "Loading in Video 4Y8CsLUY3oI.mkv (3 of 37)\n",
      "[==================================================] 100.00% 20/20\n",
      "Loading in Video 93akqaEpFLA.mp4 (4 of 37)\n",
      "[==================================================] 100.00% 20/20\n",
      "Loading in Video AHHFUPcv3Wk.mkv (5 of 37)\n",
      "[==================================================] 100.00% 20/20\n",
      "Loading in Video AThrM2iWA2o.mkv (6 of 37)\n",
      "[==================================================] 100.00% 20/20\n",
      "Loading in Video b1wRBGuXqds.mp4 (7 of 37)\n",
      "[==================================================] 100.00% 20/20\n",
      "Loading in Video B2oWrpVLsJ4.mkv (8 of 37)\n",
      "[==................................................] 5.00% 1/20"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15576/348270665.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mframe_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mRCNN_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRCNN_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;31m# Getting humans only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\RCNN\\model\\Mask_RCNN-tensorflow2.0\\mrcnn\\model.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, images, verbose)\u001b[0m\n\u001b[0;32m   2515\u001b[0m         \u001b[1;31m# Run object detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2516\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmrcnn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2517\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmolded_images\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2518\u001b[0m         \u001b[1;31m# Process detections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2519\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 988\u001b[1;33m     return func.predict(\n\u001b[0m\u001b[0;32m    989\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    701\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[0;32m    702\u001b[0m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[1;32m--> 703\u001b[1;33m     return predict_loop(\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4052\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4054\u001b[1;33m     fetched = self._callable_fn(*array_vals,\n\u001b[0m\u001b[0;32m   4055\u001b[0m                                 run_metadata=self.run_metadata)\n\u001b[0;32m   4056\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0m\u001b[0;32m   1481\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                                run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if OUTPUT:\n",
    "    print(\"Starting Symbolization Process\")\n",
    "for i, video_path in enumerate(video_paths):\n",
    "    if OUTPUT:\n",
    "        print(f\"\\nLoading in Video {video_path} ({i} of {num_vids})\")\n",
    "    video = cv2.VideoCapture(os.path.join(VIDEO_FOLDER, video_path))\n",
    "    tracker = sort.Sort() # Using default tracker settings\n",
    "    sort.KalmanBoxTracker.count = 0\n",
    "    # Dictionary to store symbols for each person\n",
    "    human_tracked_symbols = dict()\n",
    "    human_tracked_image = dict() # An image of the person being tracked\n",
    "    if OUTPUT:\n",
    "        frame_count = 0\n",
    "        total_frames = 20 #int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        percentage = frame_count / total_frames * 100\n",
    "        sys.stdout.write(\"\\r[{}{}] {:.2f}% {}/{}\".format(\n",
    "                                            '='*int(percentage/2),\n",
    "                                            '.' *(50 - int(percentage/2)),\n",
    "                                            percentage, frame_count,\n",
    "                                            total_frames))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    if not os.path.exists(OUTPUT_FOLDER):\n",
    "        os.mkdir(OUTPUT_FOLDER)\n",
    "\n",
    "    if not os.path.exists(os.path.join(OUTPUT_FOLDER, video_path.split('.')[0])):\n",
    "        os.mkdir(os.path.join(OUTPUT_FOLDER, video_path.split('.')[0]))\n",
    "    else:\n",
    "        print(f\"\\nSkipping video {video_path}\")\n",
    "        continue\n",
    "\n",
    "    while frame_count < 20:\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if OUTPUT:\n",
    "            frame_count += 1\n",
    "            percentage = frame_count / total_frames * 100\n",
    "        if ret:\n",
    "            frame_height, frame_width, _ = frame.shape\n",
    "            RCNN_results = model.detect([frame])\n",
    "            r = RCNN_results[0]\n",
    "            # Getting humans only\n",
    "            indices = np.where(r['scores'] >= 0.90)\n",
    "            indices = np.where(r['class_ids'][indices[0]] == 1)\n",
    "            all_humans_bbox = []\n",
    "            for i, index in enumerate(indices[0]):\n",
    "                y1, x1, y2, x2 = r['rois'][index]\n",
    "                bbox = (x1, y1, x2, y2, r['scores'][index])\n",
    "                all_humans_bbox.append(bbox)\n",
    "\n",
    "            if all_humans_bbox == []:\n",
    "                output = tracker.update()\n",
    "                # Maybe you should add a symbol to all current tracked humans\n",
    "            else:\n",
    "                output = tracker.update(np.array(all_humans_bbox))\n",
    "            for human in output:\n",
    "                pose.__init__(min_detection_confidence=0.5,\n",
    "                                    min_tracking_confidence=0.5)\n",
    "                x1 = int(human[0])\n",
    "                y1 = int(human[1])\n",
    "                x2 = int(human[2])\n",
    "                y2 = int(human[3])\n",
    "                key = human[4]\n",
    "\n",
    "                if (x1 < 0 or y1 < 0 or x2 < 0 or y2 < 0):\n",
    "                    if key not in human_tracked_symbols.keys():\n",
    "                        # Storing left and right symbols\n",
    "                        human_tracked_symbols[key] = ([\"H\"],\n",
    "                                                      [\"H\"],\n",
    "                                                      [str(frame_count)])\n",
    "                    else:\n",
    "                        human_tracked_symbols[key][0].append(\"H\")\n",
    "                        human_tracked_symbols[key][1].append(\"H\")\n",
    "                        human_tracked_symbols[key][2].append(str(frame_count))\n",
    "\n",
    "                    if key not in human_tracked_image.keys():\n",
    "                        human_tracked_image[key] = frame[y1:y2, x1:x2]\n",
    "                    continue\n",
    "                person_frame = frame[y1:y2, x1:x2].copy()\n",
    "                height, width, _ = person_frame.shape\n",
    "                mp_pose_results = pose.process(person_frame)\n",
    "                if (mp_pose_results.pose_landmarks == None):\n",
    "                    if key not in human_tracked_symbols.keys():\n",
    "                        # Storing left and right symbols\n",
    "                        human_tracked_symbols[key] = ([\"H\"],\n",
    "                                                      [\"H\"],\n",
    "                                                      [str(frame_count)])\n",
    "                    else:\n",
    "                        human_tracked_symbols[key][0].append(\"H\")\n",
    "                        human_tracked_symbols[key][1].append(\"H\")\n",
    "                        human_tracked_symbols[key][2].append(str(frame_count))\n",
    "\n",
    "                    if key not in human_tracked_image.keys():\n",
    "                        human_tracked_image[key] = frame[y1:y2, x1:x2]\n",
    "                    continue\n",
    "\n",
    "                left_hand = get_landmark_values(hand_landmarks_left,\n",
    "                                                mp_pose_results)\n",
    "                right_hand = get_landmark_values(hand_landmarks_right,\n",
    "                                                 mp_pose_results)\n",
    "                mouth = get_landmark_values(mouth_landmarks,\n",
    "                                            mp_pose_results)\n",
    "                eyes = get_landmark_values(eye_landmarks,\n",
    "                                           mp_pose_results)\n",
    "                shoulder = get_landmark_values(shoulder_landmarks,\n",
    "                                               mp_pose_results)\n",
    "                hip = get_landmark_values(hip_landmarks,\n",
    "                                          mp_pose_results)\n",
    "                shoulder_hip_distance = (hip[0] - shoulder[0],\n",
    "                                         hip[1] - shoulder[1])\n",
    "\n",
    "                third_shoulder_hip = (shoulder[0] +\\\n",
    "                                        shoulder_hip_distance[0]/3,\n",
    "                                      shoulder[1] +\\\n",
    "                                        shoulder_hip_distance[1]/3)\n",
    "\n",
    "                two_third_shoulder_hip = (shoulder[0] +\\\n",
    "                                            2*shoulder_hip_distance[0]/3,\n",
    "                                          shoulder[1] +\\\n",
    "                                            2*shoulder_hip_distance[1]/3)\n",
    "                symbols[\"A\"] = (eyes[1], 0)\n",
    "                symbols[\"B\"] = (mouth[1], eyes[1])\n",
    "                symbols[\"C\"] = (shoulder[1], mouth[1])\n",
    "                symbols[\"D\"] = (third_shoulder_hip[1], shoulder[1])\n",
    "                symbols[\"E\"] = (two_third_shoulder_hip[1],\n",
    "                                third_shoulder_hip[1])\n",
    "                symbols[\"F\"] = (hip[1], third_shoulder_hip[1])\n",
    "                symbols[\"G\"] = (height, hip[1])\n",
    "                #Check which region hands are in\n",
    "                left_flag = False\n",
    "                right_flag = False\n",
    "                left_symbol = \"\"\n",
    "                right_symbol = \"\"\n",
    "                for symbol in symbols.keys():\n",
    "                    if check(symbols[symbol],\n",
    "                             left_hand[1]) and not left_flag:\n",
    "                        left_symbol = symbol\n",
    "                        left_flag = True\n",
    "                        if VIS:\n",
    "                            person_frame = cv2.circle(person_frame,\n",
    "                                                (int(left_hand[0]*width),\n",
    "                                                  int(left_hand[1]*height)),\n",
    "                                                2,\n",
    "                                                colors[symbol],\n",
    "                                                2)\n",
    "\n",
    "                    if check(symbols[symbol],\n",
    "                             right_hand[1]) and not right_flag:\n",
    "                        right_symbol = symbol\n",
    "                        right_flag = True\n",
    "                        if VIS:\n",
    "                            person_frame = cv2.circle(person_frame,\n",
    "                                                (int(right_hand[0]*width),\n",
    "                                                 int(right_hand[1]*height)),\n",
    "                                                2,\n",
    "                                                colors[symbol],\n",
    "                                                2)\n",
    "\n",
    "                    if left_flag and right_flag:\n",
    "                        break\n",
    "                if key not in human_tracked_symbols.keys():\n",
    "                    # Storing left and right symbols\n",
    "                    human_tracked_symbols[key] = ([left_symbol],\n",
    "                                                  [right_symbol],\n",
    "                                                  [str(frame_count)])\n",
    "                else:\n",
    "                    human_tracked_symbols[key][0].append(left_symbol)\n",
    "                    human_tracked_symbols[key][1].append(right_symbol)\n",
    "                    human_tracked_symbols[key][2].append(str(frame_count))\n",
    "\n",
    "                if key not in human_tracked_image.keys():\n",
    "                    human_tracked_image[key] = frame[y1:y2, x1:x2]\n",
    "\n",
    "                if VIS:\n",
    "                    person_frame = cv2.line(person_frame,\n",
    "                                    (0,\n",
    "                                     int(third_shoulder_hip[1]*height)),\n",
    "                                    (int(width),\n",
    "                                     int(third_shoulder_hip[1]*height)),\n",
    "                                    (0, 255, 0),\n",
    "                                    thickness=2)\n",
    "                    person_frame = cv2.line(person_frame,\n",
    "                                    (0,\n",
    "                                     int(two_third_shoulder_hip[1]*height)),\n",
    "                                    (int(width),\n",
    "                                     int(two_third_shoulder_hip[1]*height)),\n",
    "                                    (0, 255, 0),\n",
    "                                    thickness=2)\n",
    "                    person_frame = cv2.line(person_frame,\n",
    "                                    (0,\n",
    "                                     int(shoulder[1]*height)),\n",
    "                                    (int(width),\n",
    "                                     int(shoulder[1]*height)),\n",
    "                                    (0, 255, 0),\n",
    "                                    thickness=2)\n",
    "                    person_frame = cv2.line(person_frame,\n",
    "                                    (0,\n",
    "                                     int(hip[1]*height)),\n",
    "                                    (int(width),\n",
    "                                     int(hip[1]*height)),\n",
    "                                    (0, 255, 0),\n",
    "                                    thickness=2)\n",
    "                    person_frame = cv2.line(person_frame,\n",
    "                                    (0,\n",
    "                                     int(eyes[1]*height)),\n",
    "                                    (int(width),\n",
    "                                     int(eyes[1]*height)),\n",
    "                                    (0, 255, 0),\n",
    "                                    thickness=2)\n",
    "                    person_frame = cv2.line(person_frame,\n",
    "                                    (0,\n",
    "                                     int(mouth[1]*height)),\n",
    "                                    (int(width),\n",
    "                                     int(mouth[1]*height)),\n",
    "                                    (0, 255, 0),\n",
    "                                    thickness=2)\n",
    "                    cv2.imshow(str(key), person_frame)\n",
    "                    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                        break\n",
    "            if OUTPUT:\n",
    "                sys.stdout.write(\"\\r[{}{}] {:.2f}% {}/{}\".format('='*int(percentage/2),\n",
    "                                                    '.' *(50 - int(percentage/2)),\n",
    "                                                    percentage, frame_count,\n",
    "                                                    total_frames))\n",
    "                sys.stdout.flush()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for key in human_tracked_image.keys():\n",
    "        symbol_file = open(os.path.join(OUTPUT_FOLDER, video_path.split('.')[0], f\"{video_path.split('.')[0]}.{int(key)}.txt\"), 'w')\n",
    "        symbol_file.write(f\"frame:{','.join(human_tracked_symbols[key][2])}\\nleft:{','.join(human_tracked_symbols[key][0])}\\nright:{','.join(human_tracked_symbols[key][1])}\\n\")\n",
    "        symbol_file.close()\n",
    "        if human_tracked_image[key].size != 0:\n",
    "            cv2.imwrite(os.path.join(OUTPUT_FOLDER, video_path.split('.')[0], f\"{video_path.split('.')[0]}.{int(key)}.jpg\"), human_tracked_image[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5394f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
